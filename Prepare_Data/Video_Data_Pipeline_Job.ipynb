{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6948566f-b9bf-4439-bab2-42f0ac29657c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Video Data Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb22501-7086-4743-9d02-677df797b902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook demonstrates an end-to-end pipeline for processing video data using Databricks, focusing on:\n",
    "\n",
    "1. Loading video files (.mp4, .mov) from a source volume\n",
    "2. Transcribing the videos audio to text using Whisper AI\n",
    "3. Chunking the transcribed text into manageable segments\n",
    "4. Creating a vector search index for efficient retrieval\n",
    "\n",
    "The notebook is structured in sequential steps, from data ingestion through to indexing, making it easy to understand and modify for your specific video processing needs. Each major section is clearly commented and includes relevant configuration parameters.\n",
    "\n",
    "Key components used:\n",
    "- Databricks Vector Search\n",
    "- FFMpeg package\n",
    "- Whisper AI for transcription\n",
    "- BGE embedding model for text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27468449-8d5f-4d46-af84-6246a0a79a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Setup and Configuration\n",
    " \n",
    "This section defines key configuration parameters used throughout the notebook:\n",
    " \n",
    " - Unity Catalog settings (catalog, schema, volume names)\n",
    " - Model endpoints (Whisper AI, BGE embeddings) \n",
    " - Delta table names\n",
    " - Vector search configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f3f19f-9c03-4b0f-b3a4-2fdcbe209c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Packages required by all code.\n",
    "# Versions of Databricks code are not locked since Databricks ensures changes are backwards compatible.\n",
    "# Versions of open source packages are locked since package authors often make backwards compatible changes\n",
    "%pip install -qqqq -U \\\n",
    "  databricks-vectorsearch databricks-agents pydantic databricks-sdk mlflow mlflow-skinny `# For agent & data pipeline code` \\\n",
    "  pypdf==4.1.0  `# PDF parsing` \\\n",
    "  markdownify==0.12.1  `# HTML parsing` \\\n",
    "  pypandoc_binary==1.13  `# DOCX parsing` \\\n",
    "  transformers==4.41.1 torch==2.3.0 tiktoken==0.7.0 langchain-text-splitters==0.2.0. `# get_recursive_character_text_splitter`\n",
    "\n",
    "# Restart to load the packages into the Python environment\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17cfca0-441a-428b-87ea-c93580380891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../global_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f18029f-4f6a-4ef1-9317-221cc7a5ed50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Video Processing and Transcription Pipeline Documentation\n",
    "\n",
    "## Overview\n",
    "This code implements a robust video processing and transcription pipeline that handles video files, extracts audio, processes it in chunks, and generates transcripts using Whisper. The pipeline is designed to work efficiently with large video files while managing system resources effectively.\n",
    "\n",
    "## Key Features\n",
    "- **Video Format Support**: Handles multiple video formats including MP4, AVI, MOV, MKV, WebM, FLV, WMV, MPEG, 3GP\n",
    "- **Resource Management**: Monitors and manages system resources (CPU, Memory) during processing\n",
    "- **Chunked Processing**: Splits large videos into manageable chunks for efficient processing\n",
    "- **Parallel Processing**: Processes chunks in batches for better performance\n",
    "- **Error Handling**: Comprehensive error handling and logging throughout the pipeline\n",
    "- **Delta Lake Integration**: Stores results in Delta Lake tables for efficient querying and management\n",
    "\n",
    "## Process Flow\n",
    "1. **Video File Discovery**\n",
    "   - Scans specified directory for supported video files\n",
    "   - Validates file formats and existence\n",
    "\n",
    "2. **Metadata Extraction**\n",
    "   - Extracts video duration, format, and other metadata\n",
    "   - Calculates optimal chunk size based on video duration\n",
    "\n",
    "3. **Audio Extraction and Chunking**\n",
    "   - Extracts audio from video files\n",
    "   - Splits audio into optimal-sized chunks\n",
    "   - Processes chunks in batches to manage memory usage\n",
    "\n",
    "4. **Transcription Processing**\n",
    "   - Processes each audio chunk through Whisper\n",
    "   - Combines chunk transcripts in correct order\n",
    "   - Stores results in Delta Lake table\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Resource Management\n",
    "```python\n",
    "MAX_MEMORY_PERCENT = 80  # Maximum memory usage percentage\n",
    "BATCH_SIZE = 7  # Number of chunks to process in each batch\n",
    "MIN_CHUNK_DURATION = 30  # Minimum chunk duration in seconds\n",
    "MAX_CHUNK_DURATION = 60  # Maximum chunk duration in seconds\n",
    "```\n",
    "\n",
    "### Audio Processing Parameters\n",
    "- Sample Rate: 16000 Hz\n",
    "- Channels: Mono\n",
    "- Codec: libmp3lame\n",
    "- Bitrate: 64k\n",
    "\n",
    "### Data Structures\n",
    "The pipeline uses several key data structures:\n",
    "1. **Video Metadata**\n",
    "   - Duration\n",
    "   - Name\n",
    "   - Path\n",
    "   - Format\n",
    "   - File Format\n",
    "\n",
    "2. **Chunk Data**\n",
    "   - Start Time\n",
    "   - End Time\n",
    "   - Size\n",
    "   - Binary Content\n",
    "\n",
    "3. **Output DataFrame Schema**\n",
    "   - Modality\n",
    "   - Name\n",
    "   - Path\n",
    "   - Length\n",
    "   - Modification Time\n",
    "   - Transcript Text\n",
    "   - Chunk Count\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "#### `extract_and_chunk_audio(video_path, chunk_duration_seconds=None)`\n",
    "- Extracts audio from video\n",
    "- Splits into optimal-sized chunks\n",
    "- Returns chunk data and video metadata\n",
    "\n",
    "#### `process_chunk_batch(video_path, start_times, durations)`\n",
    "- Processes multiple chunks in parallel\n",
    "- Manages system resources\n",
    "- Returns processed chunk data\n",
    "\n",
    "#### `combine_transcripts(spark, transcript_df)`\n",
    "- Combines individual chunk transcripts\n",
    "- Maintains proper ordering\n",
    "- Creates final transcript\n",
    "\n",
    "## Usage Notes\n",
    "\n",
    "### Prerequisites\n",
    "- FFmpeg installed and available in system path\n",
    "- PySpark environment configured\n",
    "- Access to Whisper endpoint\n",
    "- Sufficient system resources\n",
    "\n",
    "### Performance Considerations\n",
    "- Chunk size affects memory usage and processing speed\n",
    "- Batch size can be adjusted based on available resources\n",
    "- System resource monitoring prevents memory overflow\n",
    "\n",
    "### Error Handling\n",
    "- Comprehensive logging throughout the pipeline\n",
    "- Graceful handling of unsupported formats\n",
    "- Resource limit monitoring and pausing\n",
    "\n",
    "### Output\n",
    "- Results stored in Delta Lake table\n",
    "- Includes full transcript and metadata\n",
    "- Maintains original video information\n",
    "\n",
    "## Limitations\n",
    "- Maximum chunk duration of 60 seconds\n",
    "- Minimum chunk duration of 30 seconds\n",
    "- Memory usage capped at 80%\n",
    "- Batch processing limited to 7 chunks\n",
    "\n",
    "## Future Improvements\n",
    "- Dynamic batch size adjustment\n",
    "- Support for additional video formats\n",
    "- Enhanced error recovery mechanisms\n",
    "- Parallel processing optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134a3498-5cb8-42b0-aef8-103b1befe3f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Video Metadata Extraction and Chunk Processing"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, DoubleType, BinaryType\n",
    "from pyspark.sql.functions import collect_list, struct, col, concat_ws, count, lit\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from pyspark.sql.window import Window\n",
    "import tempfile\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants for resource monitoring\n",
    "MAX_MEMORY_PERCENT = 80  # Maximum memory usage percentage\n",
    "BATCH_SIZE = 7  # Number of chunks to process in each batch\n",
    "MIN_CHUNK_DURATION = 30  # Minimum chunk duration in seconds\n",
    "MAX_CHUNK_DURATION = 60  # Maximum chunk duration in seconds\n",
    "\n",
    "# Supported video formats\n",
    "SUPPORTED_VIDEO_FORMATS = {\n",
    "    '.mp4': 'MP4',\n",
    "    '.avi': 'AVI',\n",
    "    '.mov': 'MOV',\n",
    "    '.mkv': 'MKV',\n",
    "    '.webm': 'WebM',\n",
    "    '.flv': 'FLV',\n",
    "    '.wmv': 'WMV',\n",
    "    '.mpg': 'MPEG',\n",
    "    '.mpeg': 'MPEG',\n",
    "    '.3gp': '3GP'\n",
    "}\n",
    "\n",
    "def get_resource_usage():\n",
    "    \"\"\"Get current resource usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    return {\n",
    "        'memory_percent': process.memory_percent(),\n",
    "        'memory_used': memory_info.rss / (1024 * 1024),  # MB\n",
    "        'cpu_percent': process.cpu_percent(interval=1)\n",
    "    }\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"Ensure the directory exists, create if it doesn't\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_video_format(video_path):\n",
    "    \"\"\"\n",
    "    Get the format of the video file\n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "    Returns:\n",
    "        str: Video format or None if not supported\n",
    "    \"\"\"\n",
    "    file_ext = os.path.splitext(video_path)[1].lower()\n",
    "    return SUPPORTED_VIDEO_FORMATS.get(file_ext)\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"\n",
    "    Get metadata about the video file\n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "    Returns:\n",
    "        dict: Video metadata including duration, name, path, and format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get format information\n",
    "        format_cmd = [\n",
    "            'ffprobe',\n",
    "            '-i', video_path,\n",
    "            '-show_entries', 'format=format_name',\n",
    "            '-v', 'quiet',\n",
    "            '-of', 'csv=p=0'\n",
    "        ]\n",
    "        format_name = subprocess.check_output(format_cmd).decode().strip()\n",
    "        \n",
    "        # Get duration\n",
    "        duration_cmd = [\n",
    "            'ffprobe',\n",
    "            '-i', video_path,\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-v', 'quiet',\n",
    "            '-of', 'csv=p=0'\n",
    "        ]\n",
    "        duration = float(subprocess.check_output(duration_cmd).decode().strip())\n",
    "        \n",
    "        return {\n",
    "            'duration': duration,\n",
    "            'name': os.path.basename(video_path),\n",
    "            'path': video_path,\n",
    "            'format': format_name,\n",
    "            'file_format': get_video_format(video_path)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting video metadata: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_chunk(video_path, start_time, duration):\n",
    "    \"\"\"\n",
    "    Process a single chunk and return its binary content\n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        start_time: Start time in seconds\n",
    "        duration: Duration in seconds\n",
    "    Returns:\n",
    "        bytes: Binary content of the audio chunk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a temporary file to store the chunk\n",
    "        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=True) as temp_file:\n",
    "            cmd = [\n",
    "                'ffmpeg',\n",
    "                '-y',  # Overwrite output files\n",
    "                '-i', video_path,\n",
    "                '-ss', str(start_time),\n",
    "                '-t', str(duration),\n",
    "                '-vn',                 # No video\n",
    "                '-acodec', 'libmp3lame',  # MP3 codec\n",
    "                '-ar', '16000',       # Sample rate\n",
    "                '-ac', '1',           # Mono channel\n",
    "                '-b:a', '64k',        # Lower bitrate\n",
    "                temp_file.name\n",
    "            ]\n",
    "            \n",
    "            logger.info(f\"Processing chunk at {start_time}s\")\n",
    "            subprocess.run(cmd, check=True)\n",
    "            \n",
    "            # Read the binary content\n",
    "            with open(temp_file.name, 'rb') as f:\n",
    "                binary_content = f.read()\n",
    "            \n",
    "            return binary_content\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Error processing chunk: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def check_resources():\n",
    "    \"\"\"Check if system resources are within limits\"\"\"\n",
    "    usage = get_resource_usage()\n",
    "    if usage['memory_percent'] > MAX_MEMORY_PERCENT:\n",
    "        logger.warning(f\"High memory usage: {usage['memory_percent']}%\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def calculate_optimal_chunk_duration(total_duration):\n",
    "    \"\"\"\n",
    "    Calculate optimal chunk duration based on video length\n",
    "    Args:\n",
    "        total_duration: Total video duration in seconds\n",
    "    Returns:\n",
    "        int: Optimal chunk duration in seconds\n",
    "    \"\"\"\n",
    "    if total_duration <= 300:  # 5 minutes\n",
    "        return MIN_CHUNK_DURATION\n",
    "    elif total_duration <= 1800:  # 30 minutes\n",
    "        return 60\n",
    "    else:  # > 30 minutes\n",
    "        return min(MAX_CHUNK_DURATION, total_duration / 20)  # Aim for ~20 chunks\n",
    "\n",
    "def process_chunk_batch(video_path, start_times, durations):\n",
    "    \"\"\"\n",
    "    Process a batch of chunks in parallel\n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        start_times: List of start times for chunks\n",
    "        durations: List of durations for chunks\n",
    "    Returns:\n",
    "        list: List of binary contents for each chunk\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for start_time, duration in zip(start_times, durations):\n",
    "        if not check_resources():\n",
    "            logger.warning(\"Resource limits exceeded, pausing processing\")\n",
    "            time.sleep(5)  # Wait for resources to free up\n",
    "            if not check_resources():\n",
    "                raise Exception(\"Resource limits exceeded after waiting\")\n",
    "        \n",
    "        binary_content = process_chunk(video_path, start_time, duration)\n",
    "        if binary_content:\n",
    "            results.append({\n",
    "                'start_time': start_time,\n",
    "                'end_time': start_time + duration,\n",
    "                'size': len(binary_content),\n",
    "                'binary_content': binary_content\n",
    "            })\n",
    "        else:\n",
    "            logger.error(f\"Failed to process chunk starting at {start_time}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_and_chunk_audio(video_path, chunk_duration_seconds=None):\n",
    "    \"\"\"\n",
    "    Extract audio from video and split into chunks in memory\n",
    "    Args:\n",
    "        video_path: Path to the video file in Databricks volume\n",
    "        chunk_duration_seconds: Duration of each chunk in seconds (optional)\n",
    "    Returns:\n",
    "        tuple: (success_status, list of chunk data or error_message, video_metadata)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get video metadata\n",
    "        video_metadata = get_video_metadata(video_path)\n",
    "        if not video_metadata:\n",
    "            return False, \"Failed to get video metadata\", None\n",
    "            \n",
    "        # Verify video file exists\n",
    "        if not os.path.exists(video_path):\n",
    "            return False, f\"Video file not found at: {video_path}\", None\n",
    "        \n",
    "        # Calculate optimal chunk duration if not provided\n",
    "        if chunk_duration_seconds is None:\n",
    "            chunk_duration_seconds = calculate_optimal_chunk_duration(video_metadata['duration'])\n",
    "        \n",
    "        # Calculate number of chunks\n",
    "        total_duration = video_metadata['duration']\n",
    "        num_chunks = math.ceil(total_duration / chunk_duration_seconds)\n",
    "        logger.info(f\"Will create {num_chunks} chunks of {chunk_duration_seconds} seconds each\")\n",
    "        \n",
    "        chunk_data = []\n",
    "        failed_chunks = []\n",
    "        \n",
    "        # Process chunks in batches\n",
    "        for batch_start in range(0, num_chunks, BATCH_SIZE):\n",
    "            batch_end = min(batch_start + BATCH_SIZE, num_chunks)\n",
    "            logger.info(f\"Processing batch {batch_start//BATCH_SIZE + 1} of {math.ceil(num_chunks/BATCH_SIZE)}\")\n",
    "            \n",
    "            start_times = [i * chunk_duration_seconds for i in range(batch_start, batch_end)]\n",
    "            durations = [chunk_duration_seconds] * (batch_end - batch_start)\n",
    "            \n",
    "            try:\n",
    "                batch_results = process_chunk_batch(video_path, start_times, durations)\n",
    "                chunk_data.extend(batch_results)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch: {str(e)}\")\n",
    "                failed_chunks.extend(range(batch_start, batch_end))\n",
    "                continue\n",
    "        \n",
    "        # Report results\n",
    "        logger.info(\"\\nProcessing Summary:\")\n",
    "        logger.info(f\"Total chunks attempted: {num_chunks}\")\n",
    "        logger.info(f\"Successfully created: {len(chunk_data)}\")\n",
    "        logger.info(f\"Failed chunks: {failed_chunks}\")\n",
    "        \n",
    "        if not chunk_data:\n",
    "            return False, \"Failed to create any chunks\", None\n",
    "            \n",
    "        return True, chunk_data, video_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing video: {str(e)}\")\n",
    "        return False, f\"Error processing video: {str(e)}\", None\n",
    "\n",
    "def create_audio_dataframe(spark, video_metadata, chunk_data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with the audio chunks in memory\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        video_metadata: Dictionary containing video metadata\n",
    "        chunk_data: List of dictionaries containing chunk information\n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with audio chunks in memory\n",
    "    \"\"\"\n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"path\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"modificationTime\", StringType(), True),\n",
    "        StructField(\"chunk_index\", IntegerType(), True),\n",
    "        StructField(\"chunk_start_time\", DoubleType(), True),\n",
    "        StructField(\"chunk_end_time\", DoubleType(), True),\n",
    "        StructField(\"chunk_size_bytes\", IntegerType(), True),\n",
    "        StructField(\"audio_binary\", BinaryType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame with chunks\n",
    "    from pyspark.sql import Row\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    rows = [\n",
    "        Row(\n",
    "            name=video_metadata['name'],\n",
    "            path=video_metadata['path'],\n",
    "            length=float(video_metadata['duration']),\n",
    "            modificationTime=timestamp,\n",
    "            chunk_index=i,\n",
    "            chunk_start_time=float(chunk['start_time']),\n",
    "            chunk_end_time=float(chunk['end_time']),\n",
    "            chunk_size_bytes=chunk['size'],\n",
    "            audio_binary=chunk['binary_content']\n",
    "        )\n",
    "        for i, chunk in enumerate(chunk_data)\n",
    "    ]\n",
    "    \n",
    "    return spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "def combine_transcripts(spark, transcript_df):\n",
    "    \"\"\"\n",
    "    Combine transcript chunks into a single transcript per video\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        transcript_df: DataFrame containing transcript chunks\n",
    "    Returns:\n",
    "        DataFrame: Combined transcripts with original video metadata\n",
    "    \"\"\"\n",
    "    # First, sort the chunks by their index\n",
    "    sorted_df = transcript_df.orderBy(\"chunk_index\")\n",
    "    \n",
    "    # Create a window specification for ordering\n",
    "    window_spec = Window.partitionBy(\"name\").orderBy(\"chunk_index\")\n",
    "    \n",
    "    # Add a row number to ensure proper ordering\n",
    "    numbered_df = sorted_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    \n",
    "    # Group by video and collect transcripts in order\n",
    "    combined_df = numbered_df.groupBy(\n",
    "        \"name\",\n",
    "        \"path\",\n",
    "        \"length\",\n",
    "        \"modificationTime\"\n",
    "    ).agg(\n",
    "        F.collect_list(\n",
    "            F.struct(\n",
    "                F.col(\"chunk_index\"),\n",
    "                F.col(\"transcript_text\")\n",
    "            )\n",
    "        ).alias(\"chunks\"),\n",
    "        F.count(\"*\").alias(\"chunk_count\")\n",
    "    ).select(\n",
    "        # Add modality as first column\n",
    "        F.lit(\"video\").alias(\"modality\"),\n",
    "        \"name\",\n",
    "        \"path\",\n",
    "        \"length\",\n",
    "        \"modificationTime\",\n",
    "        # Sort chunks by index and combine text\n",
    "        F.concat_ws(\n",
    "            \" \",\n",
    "            F.expr(\"transform(array_sort(chunks, (left, right) -> case when left.chunk_index < right.chunk_index then -1 else 1 end), x -> x.transcript_text)\")\n",
    "        ).alias(\"transcript_text\"),\n",
    "        \"chunk_count\"\n",
    "    )\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def process_video_file(spark, video_path):\n",
    "    \"\"\"\n",
    "    Process a single video file\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        video_path: Path to the video file\n",
    "    Returns:\n",
    "        DataFrame: Combined transcripts for the video\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\nProcessing video: {video_path}\")\n",
    "    \n",
    "    # Extract and chunk audio directly in memory\n",
    "    success, result, video_metadata = extract_and_chunk_audio(video_path)\n",
    "    \n",
    "    if success:\n",
    "        # Create DataFrame with audio chunks in memory\n",
    "        audio_df = create_audio_dataframe(spark, video_metadata, result)\n",
    "        \n",
    "        # Process audio chunks with Whisper\n",
    "        logger.info(\"\\nProcessing audio chunks with Whisper...\")\n",
    "        transcript_df = audio_df.withColumn(\"modality\", lit(\"video\")) \\\n",
    "            .withColumn(\"transcript_text\", F.expr(f\"ai_query('{WHISPER_ENDPOINT_NAME}', audio_binary, failOnError => True)\"))\n",
    "        \n",
    "        # Combine transcripts\n",
    "        logger.info(\"\\nCombining transcripts...\")\n",
    "        combined_transcripts = combine_transcripts(spark, transcript_df)\n",
    "        \n",
    "        # Save combined transcripts\n",
    "        logger.info(\"\\nSaving transcripts to video_data_text table...\")\n",
    "        combined_transcripts.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{VIDEO_DATA_TABLE_NAME}\")\n",
    "        \n",
    "        # Show the combined transcripts\n",
    "        logger.info(\"\\nCombined Transcripts:\")\n",
    "        combined_transcripts.show(truncate=False)\n",
    "        \n",
    "        return combined_transcripts\n",
    "    else:\n",
    "        logger.error(f\"Error processing video {video_path}: {result}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Video files directory\n",
    "    video_dir = f\"/Volumes/{UC_CATALOG_NAME}/{UC_SCHEMA_NAME}/{UC_VOLUME_NAME}/{VIDEO_DATA_VOLUME_FOLDER}/\"\n",
    "    \n",
    "    # Get all supported video files in the directory\n",
    "    video_files = []\n",
    "    for f in os.listdir(video_dir):\n",
    "        if any(f.lower().endswith(ext) for ext in SUPPORTED_VIDEO_FORMATS.keys()):\n",
    "            video_files.append(os.path.join(video_dir, f))\n",
    "    \n",
    "    if not video_files:\n",
    "        logger.warning(f\"No supported video files found in {video_dir}\")\n",
    "        logger.info(f\"Supported formats: {', '.join(SUPPORTED_VIDEO_FORMATS.values())}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Found {len(video_files)} video files to process\")\n",
    "    \n",
    "    # Process each video file\n",
    "    all_transcripts = []\n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            # Check if video format is supported\n",
    "            video_format = get_video_format(video_path)\n",
    "            if not video_format:\n",
    "                logger.warning(f\"Skipping unsupported video format: {video_path}\")\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"\\nProcessing {video_format} video: {video_path}\")\n",
    "            transcripts = process_video_file(spark, video_path)\n",
    "            if transcripts:\n",
    "                all_transcripts.append(transcripts)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {video_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if all_transcripts:\n",
    "        # Combine all transcripts if needed\n",
    "        final_df = all_transcripts[0]\n",
    "        for df in all_transcripts[1:]:\n",
    "            final_df = final_df.union(df)\n",
    "        \n",
    "        logger.info(\"\\nFinal combined transcripts for all videos:\")\n",
    "        final_df.show(truncate=False)\n",
    "        return final_df\n",
    "    else:\n",
    "        logger.warning(\"No videos were successfully processed\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089c1363-2e0e-4f33-b149-b8aafda73e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### `get_recursive_character_text_splitter`\n",
    "\n",
    "`get_recursive_character_text_splitter` creates a new function that, given an embedding endpoint, returns a callable that can chunk text documents. This utility allows you to write the core business logic of the chunker, without dealing with the details of text splitting. You can decide to write your own, or edit this code if it does not fit your use case.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `model_serving_endpoint`: The name of the Model Serving endpoint with the embedding model.\n",
    "- `embedding_model_name`: The name of the embedding model e.g., `gte-large-en-v1.5`, etc.   If `model_serving_endpoint` is an OpenAI External Model or FMAPI model and set to `None`, this will be automatically detected. \n",
    "- `chunk_size_tokens`: An optional size for each chunk in tokens. Defaults to `None`, which uses the model's entire context window.\n",
    "- `chunk_overlap_tokens`: Tokens that should overlap between chunks. Defaults to `0`.\n",
    "\n",
    "**Returns:** A callable that takes a document (`str`) and produces a list of chunks (`list[str]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a659dafe-2459-46d3-aef4-2c51f80ddaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from typing import Callable, Tuple, Optional\n",
    "import os\n",
    "import re\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Constants\n",
    "HF_CACHE_DIR = \"/tmp/hf_cache/\"\n",
    "\n",
    "# Embedding Models Configuration\n",
    "EMBEDDING_MODELS = {\n",
    "    \"gte-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 8192,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge_large_en_v1_5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"text-embedding-ada-002\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-ada-002\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-small\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-large\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-large\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_workspace_client() -> WorkspaceClient:\n",
    "    \"\"\"Returns a WorkspaceClient instance.\"\"\"\n",
    "    return WorkspaceClient()\n",
    "\n",
    "\n",
    "def get_embedding_model_config(endpoint_type: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve embedding model configuration by endpoint type.\n",
    "    \"\"\"\n",
    "    return EMBEDDING_MODELS.get(endpoint_type)\n",
    "\n",
    "\n",
    "def extract_endpoint_type(llm_endpoint) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the endpoint type from the given llm_endpoint object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm_endpoint.config.served_entities[0].external_model.name\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return llm_endpoint.config.served_entities[0].foundation_model.name\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def detect_fmapi_embedding_model_type(\n",
    "    model_serving_endpoint: str,\n",
    ") -> Tuple[Optional[str], Optional[dict]]:\n",
    "    \"\"\"\n",
    "    Detects the embedding model type and configuration for the given endpoint.\n",
    "    Returns a tuple of (endpoint_type, embedding_config) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    client = get_workspace_client()\n",
    "\n",
    "    try:\n",
    "        llm_endpoint = client.serving_endpoints.get(name=model_serving_endpoint)\n",
    "        endpoint_type = extract_endpoint_type(llm_endpoint)\n",
    "    except Exception as e:\n",
    "        endpoint_type = None\n",
    "\n",
    "    embedding_config = (\n",
    "        get_embedding_model_config(endpoint_type) if endpoint_type else None\n",
    "    )\n",
    "    return (endpoint_type, embedding_config)\n",
    "\n",
    "\n",
    "def validate_chunk_size(chunk_spec: dict):\n",
    "    \"\"\"\n",
    "    Validate the chunk size and overlap settings in chunk_spec.\n",
    "    Raises ValueError if any condition is violated.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]\n",
    "    ) > chunk_spec[\"context_window\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed chunk_size of {chunk_spec[\"chunk_size_tokens\"]} + overlap of {chunk_spec[\"chunk_overlap_tokens\"]} '\n",
    "            f'is {chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]} which is greater than context '\n",
    "            f'window of {chunk_spec[\"context_window\"]} tokens.'\n",
    "        )\n",
    "\n",
    "    if chunk_spec[\"chunk_overlap_tokens\"] > chunk_spec[\"chunk_size_tokens\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed `chunk_overlap_tokens` of {chunk_spec[\"chunk_overlap_tokens\"]} is greater than the '\n",
    "            f'`chunk_size_tokens` of {chunk_spec[\"chunk_size_tokens\"]}. Reduce the size of `chunk_size_tokens`.'\n",
    "        )\n",
    "\n",
    "\n",
    "def get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint: str,\n",
    "    embedding_model_name: str = None,\n",
    "    chunk_size_tokens: int = None,\n",
    "    chunk_overlap_tokens: int = 0,\n",
    ") -> Callable[[str], list[str]]:\n",
    "    try:\n",
    "        # Detect the embedding model and its configuration\n",
    "        embedding_model_name, chunk_spec = detect_fmapi_embedding_model_type(\n",
    "            model_serving_endpoint\n",
    "        )\n",
    "\n",
    "        if chunk_spec is None or embedding_model_name is None:\n",
    "            # Fall back to using provided embedding_model_name\n",
    "            chunk_spec = EMBEDDING_MODELS.get(embedding_model_name)\n",
    "            if chunk_spec is None:\n",
    "                raise KeyError\n",
    "\n",
    "        # Update chunk specification based on provided parameters\n",
    "        chunk_spec[\"chunk_size_tokens\"] = (\n",
    "            chunk_size_tokens or chunk_spec[\"context_window\"]\n",
    "        )\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] = chunk_overlap_tokens\n",
    "\n",
    "        # Validate chunk size and overlap\n",
    "        validate_chunk_size(chunk_spec)\n",
    "\n",
    "        print(f'Chunk size in tokens: {chunk_spec[\"chunk_size_tokens\"]}')\n",
    "        print(f'Chunk overlap in tokens: {chunk_spec[\"chunk_overlap_tokens\"]}')\n",
    "        context_usage = (\n",
    "            round(\n",
    "                (chunk_spec[\"chunk_size_tokens\"] + chunk_spec[\"chunk_overlap_tokens\"])\n",
    "                / chunk_spec[\"context_window\"],\n",
    "                2,\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        print(\n",
    "            f'Using {context_usage}% of the {chunk_spec[\"context_window\"]} token context window.'\n",
    "        )\n",
    "\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Embedding model `{embedding_model_name}` not found. Available models: {EMBEDDING_MODELS.keys()}\"\n",
    "        )\n",
    "\n",
    "    def _recursive_character_text_splitter(text: str) -> list[str]:\n",
    "        tokenizer = chunk_spec[\"tokenizer\"]()\n",
    "        if chunk_spec[\"type\"] == \"SENTENCE_TRANSFORMER\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        elif chunk_spec[\"type\"] == \"OPENAI\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                tokenizer.name,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {chunk_spec['type']}\")\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    return _recursive_character_text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37903998-a038-467a-aa5d-d5cc1822b4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `chunk_docs`\n",
    "\n",
    "`chunk_docs` creates a new delta table, given a table of documents, computing the chunk function over each document to produce a chunked documents table. This utility will let you write the core business logic of the chunker, without dealing with the spark details. You can decide to write your own, or edit this code if it does not fit your use case.\n",
    "\n",
    "Arguments:\n",
    "- `docs_table`: The fully qualified delta table name. For example: `my_catalog.my_schema.my_docs`\n",
    "- `doc_column`: The name of the column where the documents can be found from `docs_table`. For example: `doc`.\n",
    "- `chunk_fn`: A function that takes a document (str) and produces a list of chunks (list[str]).\n",
    "- `propagate_columns`: Columns that should be propagated to the chunk table. For example: `url` to propagate the source URL.\n",
    "- `chunked_docs_table`: An optional output table name for chunks. Defaults to `{docs_table}_chunked`.\n",
    "\n",
    "Returns:\n",
    "The name of the chunked docs table.\n",
    "\n",
    "##### Examples of creating a `chunk_fn`\n",
    "\n",
    "###### Option 1: Use a recursive character text splitter.\n",
    "\n",
    "We provide a `get_recursive_character_text_splitter` util in this cookbook which will determine\n",
    "the best chunk window given the embedding endpoint that we decide to use for indexing.\n",
    "\n",
    "```py\n",
    "chunk_fn = get_recursive_character_text_splitter('databricks-bge-large-en')\n",
    "```\n",
    "\n",
    "###### Option 2: Use a custom splitter (e.g. LLamaIndex splitters)\n",
    "\n",
    "> An example `chunk_fn` using the markdown-aware node parser:\n",
    "\n",
    "```py\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "def chunk_fn(doc: str) -> list[str]:\n",
    "  documents = [Document(text=doc)]\n",
    "  nodes = parser.get_nodes_from_documents(documents)\n",
    "  return [node.get_content() for node in nodes]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d29b61-4afb-4acd-a478-14675f30dc67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Any, Callable\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as func\n",
    "from typing import Callable\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def standardize_timestamp_format(df):\n",
    "    \"\"\"\n",
    "    Standardize timestamp format in a DataFrame.\n",
    "    Converts any timestamp column to a consistent format.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    if \"modificationTime\" in df.columns:\n",
    "        return df.withColumn(\n",
    "            \"modificationTime\",\n",
    "            func.to_timestamp(func.col(\"modificationTime\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def compute_chunks(\n",
    "    docs_table: str,\n",
    "    doc_column: str,\n",
    "    chunk_fn: Callable[[str], list[str]],\n",
    "    propagate_columns: list[str],\n",
    "    chunked_docs_table: str,\n",
    "    modality: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compute chunks from a document table and append them to an existing chunked table.\n",
    "    \n",
    "    Args:\n",
    "        docs_table: Source table containing documents\n",
    "        doc_column: Column name containing the text to chunk\n",
    "        chunk_fn: Function to split text into chunks\n",
    "        propagate_columns: List of columns to propagate from the docs table to chunks table\n",
    "        chunked_docs_table: Target table for storing chunks\n",
    "        modality: Type of content (e.g., 'video', 'audio', 'pdf')\n",
    "    Returns:\n",
    "        str: Name of the chunked table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Computing chunks for `{docs_table}`...\")\n",
    "    \n",
    "    # Initialize Spark session if not already available\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Read source documents\n",
    "    raw_docs = spark.read.table(docs_table)\n",
    "    \n",
    "    # Check if modality column exists in source table\n",
    "    source_has_modality = \"modality\" in raw_docs.columns\n",
    "    \n",
    "    # Create UDF for chunking\n",
    "    parser_udf = func.udf(\n",
    "        chunk_fn,\n",
    "        returnType=ArrayType(StringType()),\n",
    "    )\n",
    "    \n",
    "    # Process documents into chunks\n",
    "    chunked_array_docs = raw_docs.withColumn(\n",
    "        \"content_chunked\", parser_udf(doc_column)\n",
    "    ).drop(doc_column)\n",
    "    \n",
    "    # Select columns to propagate, excluding modality if it exists\n",
    "    columns_to_propagate = [col for col in propagate_columns if col != \"modality\"]\n",
    "    \n",
    "    chunked_docs = chunked_array_docs.select(\n",
    "        *columns_to_propagate, explode(\"content_chunked\").alias(\"content_chunked\")\n",
    "    )\n",
    "    \n",
    "    # Add chunk_id\n",
    "    chunks_with_ids = chunked_docs.withColumn(\n",
    "        \"chunk_id\", func.md5(func.col(\"content_chunked\"))\n",
    "    )\n",
    "    \n",
    "    # Add modality column if it doesn't exist in source\n",
    "    if not source_has_modality:\n",
    "        chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Check if target table exists and get its schema\n",
    "    table_exists = spark.catalog._jcatalog.tableExists(chunked_docs_table)\n",
    "    if table_exists:\n",
    "        target_schema = spark.read.table(chunked_docs_table).schema\n",
    "        target_has_modality = \"modality\" in [field.name for field in target_schema]\n",
    "        \n",
    "        # If target has modality but source doesn't, add it\n",
    "        if target_has_modality and not source_has_modality:\n",
    "            chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Standardize timestamp format\n",
    "    chunks_with_ids = standardize_timestamp_format(chunks_with_ids)\n",
    "    \n",
    "    # Reorder columns for better display\n",
    "    final_columns = [\"chunk_id\", \"content_chunked\"]\n",
    "    if \"modality\" in chunks_with_ids.columns:\n",
    "        final_columns.append(\"modality\")\n",
    "    final_columns.extend(columns_to_propagate)\n",
    "    \n",
    "    chunks_with_ids = chunks_with_ids.select(*final_columns)\n",
    "    \n",
    "    if table_exists:\n",
    "        # Read existing chunks\n",
    "        existing_chunks = spark.read.table(chunked_docs_table)\n",
    "        \n",
    "        # Get existing chunk IDs\n",
    "        existing_ids = existing_chunks.select(\"chunk_id\").distinct()\n",
    "        \n",
    "        # Filter out chunks that already exist\n",
    "        new_chunks = chunks_with_ids.join(\n",
    "            existing_ids,\n",
    "            chunks_with_ids.chunk_id == existing_ids.chunk_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Found {chunks_with_ids.count()} total chunks, {new_chunks.count()} new chunks\")\n",
    "        \n",
    "        # Append only new chunks\n",
    "        if new_chunks.count() > 0:\n",
    "            new_chunks.write.mode(\"append\").saveAsTable(chunked_docs_table)\n",
    "            logger.info(f\"Appended {new_chunks.count()} new chunks to {chunked_docs_table}\")\n",
    "        else:\n",
    "            logger.info(\"No new chunks to append\")\n",
    "    else:\n",
    "        # Create new table if it doesn't exist\n",
    "        chunks_with_ids.write.mode(\"overwrite\").option(\n",
    "            \"overwriteSchema\", \"true\"\n",
    "        ).saveAsTable(chunked_docs_table)\n",
    "        logger.info(f\"Created new table {chunked_docs_table} with {chunks_with_ids.count()} chunks\")\n",
    "    \n",
    "    return chunked_docs_table\n",
    "\n",
    "# Example usage code\n",
    "def process_video_chunks():\n",
    "    \"\"\"\n",
    "    Example function to process video transcripts into chunks\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting video chunk processing...\")\n",
    "    \n",
    "    # Configure the chunker\n",
    "    chunk_fn = get_recursive_character_text_splitter(\n",
    "        model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "        chunk_size_tokens=384,\n",
    "        chunk_overlap_tokens=128,\n",
    "    )\n",
    "    \n",
    "    # Get source table schema\n",
    "    source_table = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{VIDEO_DATA_TABLE_NAME}\"\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    \n",
    "    # Log source table columns\n",
    "    logger.info(f\"Source table columns: {[field.name for field in source_schema]}\")\n",
    "    \n",
    "    # Get the columns to propagate\n",
    "    # Exclude only the columns we definitely don't want\n",
    "    propagate_columns = [\n",
    "        field.name\n",
    "        for field in source_schema\n",
    "        if field.name not in [\"transcript_text\", \"chunk_count\"]  # Keep name and modality\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Propagating columns: {propagate_columns}\")\n",
    "    \n",
    "    # Process chunks\n",
    "    chunked_docs_table = compute_chunks(\n",
    "        docs_table=source_table,\n",
    "        doc_column=\"transcript_text\",\n",
    "        chunk_fn=chunk_fn,\n",
    "        propagate_columns=propagate_columns,\n",
    "        chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE,\n",
    "        modality=\"video\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    result_df = spark.read.table(chunked_docs_table)\n",
    "    logger.info(f\"Chunked table schema: {result_df.schema}\")\n",
    "    logger.info(f\"Number of chunks created: {result_df.count()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# To run the processing:\n",
    "# result_df = process_video_chunks()\n",
    "# display(result_df)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Define chunking function\n",
    "def chunk_text(text: str) -> list[str]:\n",
    "    # Your chunking logic here\n",
    "    pass\n",
    "\n",
    "# Compute chunks for video transcripts\n",
    "compute_chunks(\n",
    "    docs_table=\"ankit_yadav.fluke_schema.video_data_text\",\n",
    "    doc_column=\"transcript_text\",\n",
    "    chunk_fn=chunk_text,\n",
    "    propagate_columns=[\"name\", \"path\", \"length\"],\n",
    "    chunked_docs_table=\"ankit_yadav.fluke_schema.content_chunks\",\n",
    "    modality=\"video\"\n",
    ")\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1534dd-4c41-4f2a-9476-0b6b0604ba63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure the chunker\n",
    "chunk_fn = get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "    chunk_size_tokens=384,\n",
    "    chunk_overlap_tokens=128,\n",
    ")\n",
    "\n",
    "# Get the columns from the parser except for the doc_content\n",
    "# You can modify this to adjust which fields are propagated from the docs table to the chunks table.\n",
    "propagate_columns = [\n",
    "    field.name\n",
    "    for field in spark.table(f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{VIDEO_DATA_TABLE_NAME}\").schema.fields\n",
    "    if field.name not in [\"transcript_text\", \"chunk_count\", \"name\"] #TODO Pass Name to the chunk table\n",
    "]\n",
    "\n",
    "chunked_docs_table = compute_chunks(\n",
    "    # The source documents table.\n",
    "    docs_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{VIDEO_DATA_TABLE_NAME}\",\n",
    "    # The column containing the documents to be chunked.\n",
    "    doc_column=\"transcript_text\",\n",
    "    # The chunking function that takes a string (document) and returns a list of strings (chunks).\n",
    "    chunk_fn=chunk_fn,\n",
    "    # Choose which columns to propagate from the docs table to chunks table. `doc_uri` column is required we can propagate the original document URL to the Agent's web app.\n",
    "    propagate_columns=propagate_columns,\n",
    "    # By default, the chunked_docs_table will be written to `CHUNKED_DOCS_DELTA_TABLE`.\n",
    "    chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE,\n",
    "    modality=\"video\"\n",
    ")\n",
    "\n",
    "display(spark.read.table(chunked_docs_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca4c979-f672-4b28-b1f6-4632371ebbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### `build_retriever_index`\n",
    "\n",
    "`build_retriever_index` will build the vector search index which is used by our RAG to retrieve relevant documents.\n",
    "\n",
    "Arguments:\n",
    "- `chunked_docs_table`: The chunked documents table. There is expected to be a `chunked_text` column, a `chunk_id` column, and a `url` column.\n",
    "-  `primary_key`: The column to use for the vector index primary key.\n",
    "- `embedding_source_column`: The column to compute embeddings for in the vector index.\n",
    "- `vector_search_endpoint`: An optional vector search endpoint name. It not defined, defaults to the `{table_id}_vector_search`.\n",
    "- `vector_search_index_name`: An optional index name. If not defined, defaults to `{chunked_docs_table}_index`.\n",
    "- `embedding_endpoint_name`: An embedding endpoint name.\n",
    "- `force_delete_vector_search_endpoint`: Setting this to true will rebuild the vector search endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3710d94-e2de-46ae-b64c-e91b499745f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from typing import TypedDict, Dict\n",
    "# import io\n",
    "# from typing import List, Dict, Any, Tuple, Optional, TypedDict\n",
    "# import warnings\n",
    "# import pyspark.sql.functions as func\n",
    "# from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "# from mlflow.utils import databricks_utils as du\n",
    "# from functools import partial\n",
    "# import tiktoken\n",
    "# from transformers import AutoTokenizer\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from databricks.vector_search.client import VectorSearchClient\n",
    "# import mlflow\n",
    "\n",
    "\n",
    "# def _build_index(\n",
    "#     primary_key: str,\n",
    "#     embedding_source_column: str,\n",
    "#     vector_search_endpoint: str,\n",
    "#     chunked_docs_table_name: str,\n",
    "#     vectorsearch_index_name: str,\n",
    "#     embedding_endpoint_name: str,\n",
    "#     force_delete=False,\n",
    "# ):\n",
    "\n",
    "#     # Get the vector search index\n",
    "#     vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "#     def find_index(endpoint_name, index_name):\n",
    "#         all_indexes = vsc.list_indexes(name=vector_search_endpoint).get(\n",
    "#             \"vector_indexes\", []\n",
    "#         )\n",
    "#         return vectorsearch_index_name in map(lambda i: i.get(\"name\"), all_indexes)\n",
    "\n",
    "#     if find_index(\n",
    "#         endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "#     ):\n",
    "#         if force_delete:\n",
    "#             vsc.delete_index(\n",
    "#                 endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "#             )\n",
    "#             create_index = True\n",
    "#         else:\n",
    "#             create_index = False\n",
    "#             print(\n",
    "#                 f\"Syncing index {vectorsearch_index_name}, this can take 15 minutes or much longer if you have a larger number of documents...\"\n",
    "#             )\n",
    "\n",
    "#             sync_result = vsc.get_index(index_name=vectorsearch_index_name).sync()\n",
    "\n",
    "#     else:\n",
    "#         print(\n",
    "#             f'Creating non-existent vector search index for endpoint \"{vector_search_endpoint}\" and index \"{vectorsearch_index_name}\"'\n",
    "#         )\n",
    "#         create_index = True\n",
    "\n",
    "#     if create_index:\n",
    "#         print(\n",
    "#             f\"Computing document embeddings and Vector Search Index. This can take 15 minutes or much longer if you have a larger number of documents.\"\n",
    "#         )\n",
    "\n",
    "#         vsc.create_delta_sync_index_and_wait(\n",
    "#             endpoint_name=vector_search_endpoint,\n",
    "#             index_name=vectorsearch_index_name,\n",
    "#             primary_key=primary_key,\n",
    "#             source_table_name=chunked_docs_table_name,\n",
    "#             pipeline_type=\"TRIGGERED\",\n",
    "#             embedding_source_column=embedding_source_column,\n",
    "#             embedding_model_endpoint_name=embedding_endpoint_name,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3128699-1b4e-4de9-b473-a65f700cb846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# class RetrieverIndexResult(BaseModel):\n",
    "#     vector_search_endpoint: str\n",
    "#     vector_search_index_name: str\n",
    "#     embedding_endpoint_name: str\n",
    "#     chunked_docs_table: str\n",
    "\n",
    "\n",
    "# def build_retriever_index(\n",
    "#     chunked_docs_table: str,\n",
    "#     primary_key: str,\n",
    "#     embedding_source_column: str,\n",
    "#     embedding_endpoint_name: str,\n",
    "#     vector_search_endpoint: str,\n",
    "#     vector_search_index_name: str,\n",
    "#     force_delete_vector_search_endpoint=False,\n",
    "# ) -> RetrieverIndexResult:\n",
    "\n",
    "#     retriever_index_result = RetrieverIndexResult(\n",
    "#         vector_search_endpoint=vector_search_endpoint,\n",
    "#         vector_search_index_name=vector_search_index_name,\n",
    "#         embedding_endpoint_name=embedding_endpoint_name,\n",
    "#         chunked_docs_table=chunked_docs_table,\n",
    "#     )\n",
    "\n",
    "#     # Enable CDC for Vector Search Delta Sync\n",
    "#     spark.sql(\n",
    "#         f\"ALTER TABLE {chunked_docs_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "#     )\n",
    "\n",
    "#     print(\"Building embedding index...\")\n",
    "#     # Building the index.\n",
    "#     _build_index(\n",
    "#         primary_key=primary_key,\n",
    "#         embedding_source_column=embedding_source_column,\n",
    "#         vector_search_endpoint=vector_search_endpoint,\n",
    "#         chunked_docs_table_name=chunked_docs_table,\n",
    "#         vectorsearch_index_name=vector_search_index_name,\n",
    "#         embedding_endpoint_name=embedding_endpoint_name,\n",
    "#         force_delete=force_delete_vector_search_endpoint,\n",
    "#     )\n",
    "\n",
    "#     return retriever_index_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c7e240-fbb7-4ab5-a9db-421046d8f3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retriever_index_result = build_retriever_index(\n",
    "#     # Spark requires `` to escape names with special chars, VS client does not.\n",
    "#     chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE.replace(\"`\", \"\"),\n",
    "#     primary_key=\"chunk_id\",\n",
    "#     embedding_source_column=\"content_chunked\",\n",
    "#     vector_search_endpoint=VECTOR_SEARCH_ENDPOINT,\n",
    "#     vector_search_index_name=VECTOR_INDEX_NAME,\n",
    "#     # Must match the embedding endpoint you used to chunk your documents\n",
    "#     embedding_endpoint_name=EMBEDDING_MODEL_ENDPOINT,\n",
    "#     # Set to true to re-create the vector search endpoint when re-running.\n",
    "#     force_delete_vector_search_endpoint=False,\n",
    "# )\n",
    "\n",
    "# print(retriever_index_result)\n",
    "\n",
    "# print()\n",
    "# print(\"Vector search index created! This will be used in the next notebook.\")\n",
    "# print(f\"Vector search endpoint: {retriever_index_result.vector_search_endpoint}\")\n",
    "# print(f\"Vector search index: {retriever_index_result.vector_search_index_name}\")\n",
    "# print(f\"Embedding used: {retriever_index_result.embedding_endpoint_name}\")\n",
    "# print(f\"Chunked docs table: {retriever_index_result.chunked_docs_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Video_Data_Pipeline_Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
