{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3dfab8-d103-4a81-8041-5e4268c196e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Audio Data Processing\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for processing audio data using Databricks, focusing on:\n",
    "\n",
    "1. Loading audio files (.wav) from a source volume\n",
    "2. Transcribing audio to text using Whisper AI\n",
    "3. Chunking the transcribed text into manageable segments\n",
    "4. Creating a vector search index for efficient retrieval\n",
    "\n",
    "The notebook is structured in sequential steps, from data ingestion through to indexing, making it easy to understand and modify for your specific audio processing needs. Each major section is clearly commented and includes relevant configuration parameters.\n",
    "\n",
    "Key components used:\n",
    "- Databricks Vector Search\n",
    "- Whisper AI for transcription\n",
    "- BGE embedding model for text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d378d99-38a1-471f-bc03-5822273aaa07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Setup and Configuration\n",
    " \n",
    "This section defines key configuration parameters used throughout the notebook:\n",
    " \n",
    " - Unity Catalog settings (catalog, schema, volume names)\n",
    " - Model endpoints (Whisper AI, BGE embeddings) \n",
    " - Delta table names\n",
    " - Vector search configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a217d67d-201e-41eb-8ec7-fddba404657c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installing and Updating Required Python Packages"
    }
   },
   "outputs": [],
   "source": [
    "# Packages required by all code.\n",
    "# Versions of Databricks code are not locked since Databricks ensures changes are backwards compatible.\n",
    "# Versions of open source packages are locked since package authors often make backwards compatible changes\n",
    "%pip install -qqqq -U \\\n",
    "  databricks-vectorsearch databricks-agents pydantic databricks-sdk mlflow mlflow-skinny `# For agent & data pipeline code` \\\n",
    "  transformers==4.41.1 torch==2.3.0 tiktoken==0.7.0 langchain-text-splitters==0.2.0. `# get_recursive_character_text_splitter`\n",
    "%pip install pyannote.audio torch torchaudio soundfile numpy pydub\n",
    "# Restart to load the packages into the Python environment\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa31b05-816d-4cae-bbee-cf1fd8bfb266",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading Global Configuration Settings"
    }
   },
   "outputs": [],
   "source": [
    "%run ../global_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578a1522-f1ce-49aa-bade-8f11831ad447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Loading Audio Data\n",
    "This cell reads audio files (.wav) from the specified volume path using Spark's binaryFile format. The audio files are loaded as binary data, preserving their original format for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbe1131-6236-40e0-afb1-670a160b53ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "## Audio Processing Pipeline \n",
    "\n",
    "### Overview\n",
    "This code implements an audio processing pipeline that processes audio files using PySpark and the Whisper AI model. The pipeline is designed to handle large audio files by breaking them into manageable chunks, process them through a Whisper endpoint, and store the results in a Delta table.\n",
    "\n",
    "### Key Features\n",
    "- Binary file reading and processing\n",
    "- Automatic chunking of large audio files\n",
    "- Integration with Whisper AI for transcription\n",
    "- Delta table storage of results\n",
    "- Comprehensive logging and performance monitoring\n",
    "\n",
    "### Process Flow\n",
    "1. **File Reading**: Reads audio files as binary files from a specified source path\n",
    "2. **Chunking**: Splits large audio files into optimal-sized chunks (2MB-10MB)\n",
    "3. **Processing**: Sends each chunk to a Whisper endpoint for transcription\n",
    "4. **Storage**: Saves results to a Delta table with the following columns:\n",
    "   - modality: Type of content (audio)\n",
    "   - path: Original file path\n",
    "   - modificationTime: File modification timestamp\n",
    "   - length: File size\n",
    "   - binary_content: Base64 encoded audio content\n",
    "   - transcript_text: Whisper-generated transcription\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "#### Chunking Strategy\n",
    "- Minimum chunk size: 2MB\n",
    "- Maximum chunk size: 10MB\n",
    "- Target chunks per file: 30\n",
    "- Dynamic chunk size calculation based on file size\n",
    "\n",
    "#### Performance Considerations\n",
    "- Uses PySpark for distributed processing\n",
    "- Implements efficient binary data handling\n",
    "- Includes performance logging and timing metrics\n",
    "\n",
    "#### Error Handling\n",
    "- Comprehensive error logging\n",
    "- Graceful failure handling for chunking process\n",
    "- Configurable error handling for Whisper endpoint calls\n",
    "\n",
    "### Usage Example\n",
    "```python\n",
    "process_audio_data(\n",
    "    spark=spark,\n",
    "    source_path='/Volumes/catalog/schema/volume/audio_folder/',\n",
    "    output_table=\"catalog.schema.audio_data_table\",\n",
    "    whisper_endpoint=\"whisper-endpoint-name\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Notes\n",
    "- Ensure sufficient memory allocation for processing large audio files\n",
    "- Monitor Whisper endpoint capacity and performance\n",
    "- Consider implementing retry logic for failed transcriptions\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a6a2ca-9617-478b-ad20-b902fe6454ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transcribing and Storing Audio Data with Spark"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, expr, udf, unbase64, element_at\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField, BinaryType, IntegerType, StringType\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger('audio_processor')\n",
    "\n",
    "# Suppress noisy loggers\n",
    "logging.getLogger('pyspark').setLevel(logging.WARNING)\n",
    "logging.getLogger('py4j').setLevel(logging.WARNING)\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger('PIL').setLevel(logging.WARNING)\n",
    "\n",
    "# Constants for chunking\n",
    "MIN_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB minimum chunk size\n",
    "MAX_CHUNK_SIZE = 10 * 1024 * 1024  # 10MB maximum chunk size\n",
    "TARGET_CHUNKS = 30  # Target number of chunks per file\n",
    "\n",
    "def calculate_optimal_chunk_size(file_size):\n",
    "    \"\"\"\n",
    "    Calculate optimal chunk size based on file size\n",
    "    Args:\n",
    "        file_size: Size of the file in bytes\n",
    "    Returns:\n",
    "        int: Optimal chunk size in bytes\n",
    "    \"\"\"\n",
    "    # Calculate chunk size to get approximately TARGET_CHUNKS chunks\n",
    "    chunk_size = file_size / TARGET_CHUNKS\n",
    "    \n",
    "    # Ensure chunk size is within bounds\n",
    "    return max(MIN_CHUNK_SIZE, min(chunk_size, MAX_CHUNK_SIZE))\n",
    "\n",
    "def chunk_binary_data(binary_content):\n",
    "    \"\"\"\n",
    "    Split binary data into chunks\n",
    "    Args:\n",
    "        binary_content: Binary content of the file\n",
    "    Returns:\n",
    "        list: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_size = len(binary_content)\n",
    "        chunk_size = calculate_optimal_chunk_size(file_size)\n",
    "        num_chunks = math.ceil(file_size / chunk_size)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(num_chunks):\n",
    "            start = i * chunk_size\n",
    "            end = min(start + chunk_size, file_size)\n",
    "            chunk = binary_content[start:end]\n",
    "            \n",
    "            # Encode the chunk as base64\n",
    "            encoded_chunk = base64.b64encode(chunk).decode('utf-8')\n",
    "            \n",
    "            chunks.append({\n",
    "                'chunk_index': i,\n",
    "                'size': len(chunk),\n",
    "                'binary_content': encoded_chunk  # Store as base64 encoded string\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error chunking binary data: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Define the schema for the chunked data\n",
    "chunk_schema = ArrayType(StructType([\n",
    "    StructField(\"chunk_index\", IntegerType(), True),\n",
    "    StructField(\"size\", IntegerType(), True),\n",
    "    StructField(\"binary_content\", StringType(), True)  # Changed to StringType for base64\n",
    "]))\n",
    "\n",
    "# Register the UDF\n",
    "chunk_udf = udf(chunk_binary_data, chunk_schema)\n",
    "\n",
    "def process_audio_data(\n",
    "    spark: SparkSession,\n",
    "    source_path: str,\n",
    "    output_table: str,\n",
    "    whisper_endpoint: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process audio files and create a standardized table with transcriptions.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        source_path: Path to the source audio files\n",
    "        output_table: Full name of the output table (catalog.schema.table)\n",
    "        whisper_endpoint: Name of the Whisper endpoint\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting audio processing from {source_path}\")\n",
    "    \n",
    "    # Read the audio files as binary files\n",
    "    audio_df = spark.read.format(\"binaryFile\").load(source_path)\n",
    "    file_count = audio_df.count()\n",
    "    logger.info(f\"Found {file_count} audio files to process\")\n",
    "    \n",
    "    # Add a column with the binary content\n",
    "    audio_df = audio_df.withColumn(\"binary_content\", col(\"content\"))\n",
    "    \n",
    "    # Change the data type for the length column to DoubleType\n",
    "    audio_df = audio_df.withColumn(\"length\", col(\"length\").cast(DoubleType()))\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    audio_df = audio_df.select(\"path\", \"modificationTime\", \"length\", \"binary_content\")\n",
    "    \n",
    "    # Chunk the binary content\n",
    "    chunked_df = audio_df.withColumn(\"chunks\", chunk_udf(col(\"binary_content\")))\n",
    "    logger.info(\"Successfully chunked binary content\")\n",
    "    \n",
    "    # Explode the chunks array to process each chunk separately\n",
    "    exploded_df = chunked_df.select(\n",
    "        col(\"path\"),\n",
    "        col(\"modificationTime\"),\n",
    "        col(\"length\"),\n",
    "        col(\"chunks.chunk_index\").alias(\"chunk_index\"),\n",
    "        col(\"chunks.size\").alias(\"chunk_size\"),\n",
    "        col(\"chunks.binary_content\").alias(\"binary_content\")  # This is now base64 encoded\n",
    "    )\n",
    "    chunk_count = exploded_df.count()\n",
    "    logger.info(f\"Created {chunk_count} chunks for processing\")\n",
    "    \n",
    "    # Add modality column and get transcriptions for each chunk\n",
    "    processed_df = exploded_df.select(\n",
    "        lit(\"audio\").alias(\"modality\"),\n",
    "        col(\"path\"),\n",
    "        col(\"modificationTime\"),\n",
    "        col(\"length\"),\n",
    "        # col(\"chunk_index\"),\n",
    "        # col(\"chunk_size\"),\n",
    "        col(\"binary_content\"),\n",
    "        # Convert the base64 string back to binary before sending to whisper endpoint\n",
    "        expr(f\"ai_query('{whisper_endpoint}', unbase64(element_at(binary_content, 1)), failOnError => True)\").alias(\"transcript_text\")\n",
    "    )\n",
    "    \n",
    "    # Write to Delta table\n",
    "    processed_df.write.mode(\"overwrite\").option(\n",
    "        \"overwriteSchema\", \"true\"\n",
    "    ).saveAsTable(output_table)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    logger.info(f\"Successfully processed and saved {chunk_count} audio chunks to {output_table}\")\n",
    "    logger.info(f\"Processing completed in {duration:.2f} seconds\")\n",
    "    logger.info(f\"Average processing time per file: {duration/file_count:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b8065e-6edb-460c-a9a9-cbb03ddb85dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processing Audio Data with Spark and Whisper API"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "process_audio_data(\n",
    "    spark=spark,\n",
    "    source_path=f'/Volumes/{UC_CATALOG_NAME}/{UC_SCHEMA_NAME}/{UC_VOLUME_NAME}/{AUDIO_DATA_VOLUME_FOLDER}/',\n",
    "    output_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{AUDIO_DATA_TABLE_NAME}\",\n",
    "    whisper_endpoint=WHISPER_ENDPOINT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8abb744-f018-4ec6-a065-e7fcab7456b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### `get_recursive_character_text_splitter`\n",
    "\n",
    "`get_recursive_character_text_splitter` creates a new function that, given an embedding endpoint, returns a callable that can chunk text documents. This utility allows you to write the core business logic of the chunker, without dealing with the details of text splitting. You can decide to write your own, or edit this code if it does not fit your use case.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `model_serving_endpoint`: The name of the Model Serving endpoint with the embedding model.\n",
    "- `embedding_model_name`: The name of the embedding model e.g., `gte-large-en-v1.5`, etc.   If `model_serving_endpoint` is an OpenAI External Model or FMAPI model and set to `None`, this will be automatically detected. \n",
    "- `chunk_size_tokens`: An optional size for each chunk in tokens. Defaults to `None`, which uses the model's entire context window.\n",
    "- `chunk_overlap_tokens`: Tokens that should overlap between chunks. Defaults to `0`.\n",
    "\n",
    "**Returns:** A callable that takes a document (`str`) and produces a list of chunks (`list[str]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6df3d72-fa99-43d4-98f5-7797b6391b71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuring Embedding Models and Validation Methods"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from typing import Callable, Tuple, Optional\n",
    "import os\n",
    "import re\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Constants\n",
    "HF_CACHE_DIR = \"/tmp/hf_cache/\"\n",
    "\n",
    "# Embedding Models Configuration\n",
    "EMBEDDING_MODELS = {\n",
    "    \"gte-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 8192,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge_large_en_v1_5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"text-embedding-ada-002\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-ada-002\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-small\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-large\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-large\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_workspace_client() -> WorkspaceClient:\n",
    "    \"\"\"Returns a WorkspaceClient instance.\"\"\"\n",
    "    return WorkspaceClient()\n",
    "\n",
    "\n",
    "def get_embedding_model_config(endpoint_type: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve embedding model configuration by endpoint type.\n",
    "    \"\"\"\n",
    "    return EMBEDDING_MODELS.get(endpoint_type)\n",
    "\n",
    "\n",
    "def extract_endpoint_type(llm_endpoint) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the endpoint type from the given llm_endpoint object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm_endpoint.config.served_entities[0].external_model.name\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return llm_endpoint.config.served_entities[0].foundation_model.name\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def detect_fmapi_embedding_model_type(\n",
    "    model_serving_endpoint: str,\n",
    ") -> Tuple[Optional[str], Optional[dict]]:\n",
    "    \"\"\"\n",
    "    Detects the embedding model type and configuration for the given endpoint.\n",
    "    Returns a tuple of (endpoint_type, embedding_config) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    client = get_workspace_client()\n",
    "\n",
    "    try:\n",
    "        llm_endpoint = client.serving_endpoints.get(name=model_serving_endpoint)\n",
    "        endpoint_type = extract_endpoint_type(llm_endpoint)\n",
    "    except Exception as e:\n",
    "        endpoint_type = None\n",
    "\n",
    "    embedding_config = (\n",
    "        get_embedding_model_config(endpoint_type) if endpoint_type else None\n",
    "    )\n",
    "    return (endpoint_type, embedding_config)\n",
    "\n",
    "\n",
    "def validate_chunk_size(chunk_spec: dict):\n",
    "    \"\"\"\n",
    "    Validate the chunk size and overlap settings in chunk_spec.\n",
    "    Raises ValueError if any condition is violated.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]\n",
    "    ) > chunk_spec[\"context_window\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed chunk_size of {chunk_spec[\"chunk_size_tokens\"]} + overlap of {chunk_spec[\"chunk_overlap_tokens\"]} '\n",
    "            f'is {chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]} which is greater than context '\n",
    "            f'window of {chunk_spec[\"context_window\"]} tokens.'\n",
    "        )\n",
    "\n",
    "    if chunk_spec[\"chunk_overlap_tokens\"] > chunk_spec[\"chunk_size_tokens\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed `chunk_overlap_tokens` of {chunk_spec[\"chunk_overlap_tokens\"]} is greater than the '\n",
    "            f'`chunk_size_tokens` of {chunk_spec[\"chunk_size_tokens\"]}. Reduce the size of `chunk_size_tokens`.'\n",
    "        )\n",
    "\n",
    "\n",
    "def get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint: str,\n",
    "    embedding_model_name: str = None,\n",
    "    chunk_size_tokens: int = None,\n",
    "    chunk_overlap_tokens: int = 0,\n",
    ") -> Callable[[str], list[str]]:\n",
    "    try:\n",
    "        # Detect the embedding model and its configuration\n",
    "        embedding_model_name, chunk_spec = detect_fmapi_embedding_model_type(\n",
    "            model_serving_endpoint\n",
    "        )\n",
    "\n",
    "        if chunk_spec is None or embedding_model_name is None:\n",
    "            # Fall back to using provided embedding_model_name\n",
    "            chunk_spec = EMBEDDING_MODELS.get(embedding_model_name)\n",
    "            if chunk_spec is None:\n",
    "                raise KeyError\n",
    "\n",
    "        # Update chunk specification based on provided parameters\n",
    "        chunk_spec[\"chunk_size_tokens\"] = (\n",
    "            chunk_size_tokens or chunk_spec[\"context_window\"]\n",
    "        )\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] = chunk_overlap_tokens\n",
    "\n",
    "        # Validate chunk size and overlap\n",
    "        validate_chunk_size(chunk_spec)\n",
    "\n",
    "        print(f'Chunk size in tokens: {chunk_spec[\"chunk_size_tokens\"]}')\n",
    "        print(f'Chunk overlap in tokens: {chunk_spec[\"chunk_overlap_tokens\"]}')\n",
    "        context_usage = (\n",
    "            round(\n",
    "                (chunk_spec[\"chunk_size_tokens\"] + chunk_spec[\"chunk_overlap_tokens\"])\n",
    "                / chunk_spec[\"context_window\"],\n",
    "                2,\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        print(\n",
    "            f'Using {context_usage}% of the {chunk_spec[\"context_window\"]} token context window.'\n",
    "        )\n",
    "\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Embedding model `{embedding_model_name}` not found. Available models: {EMBEDDING_MODELS.keys()}\"\n",
    "        )\n",
    "\n",
    "    def _recursive_character_text_splitter(text: str) -> list[str]:\n",
    "        tokenizer = chunk_spec[\"tokenizer\"]()\n",
    "        if chunk_spec[\"type\"] == \"SENTENCE_TRANSFORMER\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        elif chunk_spec[\"type\"] == \"OPENAI\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                tokenizer.name,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {chunk_spec['type']}\")\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    return _recursive_character_text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66be0ef-77d7-4cb5-a000-f215e1e1acc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `chunk_docs`\n",
    "\n",
    "`chunk_docs` creates a new delta table, given a table of documents, computing the chunk function over each document to produce a chunked documents table. This utility will let you write the core business logic of the chunker, without dealing with the spark details. You can decide to write your own, or edit this code if it does not fit your use case.\n",
    "\n",
    "Arguments:\n",
    "- `docs_table`: The fully qualified delta table name. For example: `my_catalog.my_schema.my_docs`\n",
    "- `doc_column`: The name of the column where the documents can be found from `docs_table`. For example: `doc`.\n",
    "- `chunk_fn`: A function that takes a document (str) and produces a list of chunks (list[str]).\n",
    "- `propagate_columns`: Columns that should be propagated to the chunk table. For example: `url` to propagate the source URL.\n",
    "- `chunked_docs_table`: An optional output table name for chunks. Defaults to `{docs_table}_chunked`.\n",
    "\n",
    "Returns:\n",
    "The name of the chunked docs table.\n",
    "\n",
    "##### Examples of creating a `chunk_fn`\n",
    "\n",
    "###### Option 1: Use a recursive character text splitter.\n",
    "\n",
    "We provide a `get_recursive_character_text_splitter` util in this cookbook which will determine\n",
    "the best chunk window given the embedding endpoint that we decide to use for indexing.\n",
    "\n",
    "```py\n",
    "chunk_fn = get_recursive_character_text_splitter('databricks-bge-large-en')\n",
    "```\n",
    "\n",
    "###### Option 2: Use a custom splitter (e.g. LLamaIndex splitters)\n",
    "\n",
    "> An example `chunk_fn` using the markdown-aware node parser:\n",
    "\n",
    "```py\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "def chunk_fn(doc: str) -> list[str]:\n",
    "  documents = [Document(text=doc)]\n",
    "  nodes = parser.get_nodes_from_documents(documents)\n",
    "  return [node.get_content() for node in nodes]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d81ec5-0ab1-4bc4-a27c-1c60b6a63ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Any, Callable\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as func\n",
    "from typing import Callable\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def standardize_timestamp_format(df):\n",
    "    \"\"\"\n",
    "    Standardize timestamp format in a DataFrame.\n",
    "    Converts any timestamp column to a consistent format.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    if \"modificationTime\" in df.columns:\n",
    "        return df.withColumn(\n",
    "            \"modificationTime\",\n",
    "            func.to_timestamp(func.col(\"modificationTime\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def compute_chunks(\n",
    "    docs_table: str,\n",
    "    doc_column: str,\n",
    "    chunk_fn: Callable[[str], list[str]],\n",
    "    propagate_columns: list[str],\n",
    "    chunked_docs_table: str,\n",
    "    modality: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compute chunks from a document table and append them to an existing chunked table.\n",
    "    \n",
    "    Args:\n",
    "        docs_table: Source table containing documents\n",
    "        doc_column: Column name containing the text to chunk\n",
    "        chunk_fn: Function to split text into chunks\n",
    "        propagate_columns: List of columns to propagate from the docs table to chunks table\n",
    "        chunked_docs_table: Target table for storing chunks\n",
    "        modality: Type of content (e.g., 'video', 'audio', 'pdf')\n",
    "    Returns:\n",
    "        str: Name of the chunked table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Computing chunks for `{docs_table}`...\")\n",
    "    \n",
    "    # Initialize Spark session if not already available\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Read source documents\n",
    "    raw_docs = spark.read.table(docs_table)\n",
    "    \n",
    "    # Check if modality column exists in source table\n",
    "    source_has_modality = \"modality\" in raw_docs.columns\n",
    "    \n",
    "    # Create UDF for chunking\n",
    "    parser_udf = func.udf(\n",
    "        chunk_fn,\n",
    "        returnType=ArrayType(StringType()),\n",
    "    )\n",
    "    \n",
    "    # Process documents into chunks\n",
    "    chunked_array_docs = raw_docs.withColumn(\n",
    "        \"content_chunked\", parser_udf(doc_column)\n",
    "    ).drop(doc_column)\n",
    "    \n",
    "    # Select columns to propagate, excluding modality if it exists\n",
    "    columns_to_propagate = [col for col in propagate_columns if col != \"modality\"]\n",
    "    \n",
    "    chunked_docs = chunked_array_docs.select(\n",
    "        *columns_to_propagate, explode(\"content_chunked\").alias(\"content_chunked\")\n",
    "    )\n",
    "    \n",
    "    # Add chunk_id\n",
    "    chunks_with_ids = chunked_docs.withColumn(\n",
    "        \"chunk_id\", func.md5(func.col(\"content_chunked\"))\n",
    "    )\n",
    "    \n",
    "    # Add modality column if it doesn't exist in source\n",
    "    if not source_has_modality:\n",
    "        chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Check if target table exists and get its schema\n",
    "    table_exists = spark.catalog._jcatalog.tableExists(chunked_docs_table)\n",
    "    if table_exists:\n",
    "        target_schema = spark.read.table(chunked_docs_table).schema\n",
    "        target_has_modality = \"modality\" in [field.name for field in target_schema]\n",
    "        \n",
    "        # If target has modality but source doesn't, add it\n",
    "        if target_has_modality and not source_has_modality:\n",
    "            chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Standardize timestamp format\n",
    "    chunks_with_ids = standardize_timestamp_format(chunks_with_ids)\n",
    "    \n",
    "    # Reorder columns for better display\n",
    "    final_columns = [\"chunk_id\", \"content_chunked\"]\n",
    "    if \"modality\" in chunks_with_ids.columns:\n",
    "        final_columns.append(\"modality\")\n",
    "    final_columns.extend(columns_to_propagate)\n",
    "    \n",
    "    chunks_with_ids = chunks_with_ids.select(*final_columns)\n",
    "    \n",
    "    if table_exists:\n",
    "        # Read existing chunks\n",
    "        existing_chunks = spark.read.table(chunked_docs_table)\n",
    "        \n",
    "        # Get existing chunk IDs\n",
    "        existing_ids = existing_chunks.select(\"chunk_id\").distinct()\n",
    "        \n",
    "        # Filter out chunks that already exist\n",
    "        new_chunks = chunks_with_ids.join(\n",
    "            existing_ids,\n",
    "            chunks_with_ids.chunk_id == existing_ids.chunk_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Found {chunks_with_ids.count()} total chunks, {new_chunks.count()} new chunks\")\n",
    "        \n",
    "        # Append only new chunks\n",
    "        if new_chunks.count() > 0:\n",
    "            new_chunks.write.mode(\"append\").saveAsTable(chunked_docs_table)\n",
    "            logger.info(f\"Appended {new_chunks.count()} new chunks to {chunked_docs_table}\")\n",
    "        else:\n",
    "            logger.info(\"No new chunks to append\")\n",
    "    else:\n",
    "        # Create new table if it doesn't exist\n",
    "        chunks_with_ids.write.mode(\"overwrite\").option(\n",
    "            \"overwriteSchema\", \"true\"\n",
    "        ).saveAsTable(chunked_docs_table)\n",
    "        logger.info(f\"Created new table {chunked_docs_table} with {chunks_with_ids.count()} chunks\")\n",
    "    \n",
    "    return chunked_docs_table\n",
    "\n",
    "# Example usage code\n",
    "def process_video_chunks():\n",
    "    \"\"\"\n",
    "    Example function to process video transcripts into chunks\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting video chunk processing...\")\n",
    "    \n",
    "    # Configure the chunker\n",
    "    chunk_fn = get_recursive_character_text_splitter(\n",
    "        model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "        chunk_size_tokens=384,\n",
    "        chunk_overlap_tokens=128,\n",
    "    )\n",
    "    \n",
    "    # Get source table schema\n",
    "    source_table = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{AUDIO_DATA_TABLE_NAME}\"\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    \n",
    "    # Log source table columns\n",
    "    logger.info(f\"Source table columns: {[field.name for field in source_schema]}\")\n",
    "    \n",
    "    # Get the columns to propagate\n",
    "    # Exclude only the columns we definitely don't want\n",
    "    propagate_columns = [\n",
    "        field.name\n",
    "        for field in source_schema\n",
    "        if field.name not in [\"transcript_text\", \"chunk_count\"]  # Keep name and modality\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Propagating columns: {propagate_columns}\")\n",
    "    \n",
    "    # Process chunks\n",
    "    chunked_docs_table = compute_chunks(\n",
    "        docs_table=source_table,\n",
    "        doc_column=\"transcript_text\",\n",
    "        chunk_fn=chunk_fn,\n",
    "        propagate_columns=propagate_columns,\n",
    "        chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE,\n",
    "        modality=\"video\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    result_df = spark.read.table(chunked_docs_table)\n",
    "    logger.info(f\"Chunked table schema: {result_df.schema}\")\n",
    "    logger.info(f\"Number of chunks created: {result_df.count()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# To run the processing:\n",
    "# result_df = process_video_chunks()\n",
    "# display(result_df)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Define chunking function\n",
    "def chunk_text(text: str) -> list[str]:\n",
    "    # Your chunking logic here\n",
    "    pass\n",
    "\n",
    "# Compute chunks for video transcripts\n",
    "compute_chunks(\n",
    "    docs_table=\"ankit_yadav.fluke_schema.video_data_text\",\n",
    "    doc_column=\"transcript_text\",\n",
    "    chunk_fn=chunk_text,\n",
    "    propagate_columns=[\"name\", \"path\", \"length\"],\n",
    "    chunked_docs_table=\"ankit_yadav.fluke_schema.content_chunks\",\n",
    "    modality=\"video\"\n",
    ")\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdce4e9-aa4a-4bdc-a455-2f5d849e0e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure the chunker\n",
    "chunk_fn = get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "    chunk_size_tokens=384,\n",
    "    chunk_overlap_tokens=128,\n",
    ")\n",
    "\n",
    "# Get the columns from the parser except for the doc_content\n",
    "# You can modify this to adjust which fields are propagated from the docs table to the chunks table.\n",
    "propagate_columns = [\n",
    "    field.name\n",
    "    for field in spark.table(f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{AUDIO_DATA_TABLE_NAME}\").schema.fields\n",
    "    if field.name not in [\"transcript_text\", \"binary_content\"]\n",
    "]\n",
    "\n",
    "chunked_docs_table = compute_chunks(\n",
    "    # The source documents table.\n",
    "    docs_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{AUDIO_DATA_TABLE_NAME}\",\n",
    "    # The column containing the documents to be chunked.\n",
    "    doc_column=\"transcript_text\",\n",
    "    # The chunking function that takes a string (document) and returns a list of strings (chunks).\n",
    "    chunk_fn=chunk_fn,\n",
    "    # Choose which columns to propagate from the docs table to chunks table. `doc_uri` column is required we can propagate the original document URL to the Agent's web app.\n",
    "    propagate_columns=propagate_columns,\n",
    "    # By default, the chunked_docs_table will be written to `{docs_table}_chunked`.\n",
    "    chunked_docs_table=f\"{CHUNKED_DOCS_DELTA_TABLE}\",\n",
    "    modality=\"video\"\n",
    ")\n",
    "\n",
    "display(spark.read.table(chunked_docs_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11832dac-0767-4e66-84cf-e5dde2365c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### `build_retriever_index`\n",
    "\n",
    "`build_retriever_index` will build the vector search index which is used by our RAG to retrieve relevant documents.\n",
    "\n",
    "Arguments:\n",
    "- `chunked_docs_table`: The chunked documents table. There is expected to be a `chunked_text` column, a `chunk_id` column, and a `url` column.\n",
    "-  `primary_key`: The column to use for the vector index primary key.\n",
    "- `embedding_source_column`: The column to compute embeddings for in the vector index.\n",
    "- `vector_search_endpoint`: An optional vector search endpoint name. It not defined, defaults to the `{table_id}_vector_search`.\n",
    "- `vector_search_index_name`: An optional index name. If not defined, defaults to `{chunked_docs_table}_index`.\n",
    "- `embedding_endpoint_name`: An embedding endpoint name.\n",
    "- `force_delete_vector_search_endpoint`: Setting this to true will rebuild the vector search endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e271225-3b14-4d2b-8577-3e94c79ffd6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Building and Syncing Vector Search Index"
    }
   },
   "outputs": [],
   "source": [
    "# from typing import TypedDict, Dict\n",
    "# import io\n",
    "# from typing import List, Dict, Any, Tuple, Optional, TypedDict\n",
    "# import warnings\n",
    "# import pyspark.sql.functions as func\n",
    "# from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "# from mlflow.utils import databricks_utils as du\n",
    "# from functools import partial\n",
    "# import tiktoken\n",
    "# from transformers import AutoTokenizer\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from databricks.vector_search.client import VectorSearchClient\n",
    "# import mlflow\n",
    "\n",
    "\n",
    "# def _build_index(\n",
    "#     primary_key: str,\n",
    "#     embedding_source_column: str,\n",
    "#     vector_search_endpoint: str,\n",
    "#     chunked_docs_table_name: str,\n",
    "#     vectorsearch_index_name: str,\n",
    "#     embedding_endpoint_name: str,\n",
    "#     force_delete=False,\n",
    "# ):\n",
    "\n",
    "#     # Get the vector search index\n",
    "#     vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "#     def find_index(endpoint_name, index_name):\n",
    "#         all_indexes = vsc.list_indexes(name=vector_search_endpoint).get(\n",
    "#             \"vector_indexes\", []\n",
    "#         )\n",
    "#         return vectorsearch_index_name in map(lambda i: i.get(\"name\"), all_indexes)\n",
    "\n",
    "#     if find_index(\n",
    "#         endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "#     ):\n",
    "#         if force_delete:\n",
    "#             vsc.delete_index(\n",
    "#                 endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "#             )\n",
    "#             create_index = True\n",
    "#         else:\n",
    "#             create_index = False\n",
    "#             print(\n",
    "#                 f\"Syncing index {vectorsearch_index_name}, this can take 15 minutes or much longer if you have a larger number of documents...\"\n",
    "#             )\n",
    "\n",
    "#             sync_result = vsc.get_index(index_name=vectorsearch_index_name).sync()\n",
    "\n",
    "#     else:\n",
    "#         print(\n",
    "#             f'Creating non-existent vector search index for endpoint \"{vector_search_endpoint}\" and index \"{vectorsearch_index_name}\"'\n",
    "#         )\n",
    "#         create_index = True\n",
    "\n",
    "#     if create_index:\n",
    "#         print(\n",
    "#             f\"Computing document embeddings and Vector Search Index. This can take 15 minutes or much longer if you have a larger number of documents.\"\n",
    "#         )\n",
    "\n",
    "#         vsc.create_delta_sync_index_and_wait(\n",
    "#             endpoint_name=vector_search_endpoint,\n",
    "#             index_name=vectorsearch_index_name,\n",
    "#             primary_key=primary_key,\n",
    "#             source_table_name=chunked_docs_table_name,\n",
    "#             pipeline_type=\"TRIGGERED\",\n",
    "#             embedding_source_column=embedding_source_column,\n",
    "#             embedding_model_endpoint_name=embedding_endpoint_name,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c6c725-ec68-4ec5-96aa-0986c46742b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Building and Configuring Retriever Index"
    }
   },
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# class RetrieverIndexResult(BaseModel):\n",
    "#     vector_search_endpoint: str\n",
    "#     vector_search_index_name: str\n",
    "#     embedding_endpoint_name: str\n",
    "#     chunked_docs_table: str\n",
    "\n",
    "\n",
    "# def build_retriever_index(\n",
    "#     chunked_docs_table: str,\n",
    "#     primary_key: str,\n",
    "#     embedding_source_column: str,\n",
    "#     embedding_endpoint_name: str,\n",
    "#     vector_search_endpoint: str,\n",
    "#     vector_search_index_name: str,\n",
    "#     force_delete_vector_search_endpoint=False,\n",
    "# ) -> RetrieverIndexResult:\n",
    "\n",
    "#     retriever_index_result = RetrieverIndexResult(\n",
    "#         vector_search_endpoint=vector_search_endpoint,\n",
    "#         vector_search_index_name=vector_search_index_name,\n",
    "#         embedding_endpoint_name=embedding_endpoint_name,\n",
    "#         chunked_docs_table=chunked_docs_table,\n",
    "#     )\n",
    "\n",
    "#     # Enable CDC for Vector Search Delta Sync\n",
    "#     spark.sql(\n",
    "#         f\"ALTER TABLE {chunked_docs_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "#     )\n",
    "\n",
    "#     print(\"Building embedding index...\")\n",
    "#     # Building the index.\n",
    "#     _build_index(\n",
    "#         primary_key=primary_key,\n",
    "#         embedding_source_column=embedding_source_column,\n",
    "#         vector_search_endpoint=vector_search_endpoint,\n",
    "#         chunked_docs_table_name=chunked_docs_table,\n",
    "#         vectorsearch_index_name=vector_search_index_name,\n",
    "#         embedding_endpoint_name=embedding_endpoint_name,\n",
    "#         force_delete=force_delete_vector_search_endpoint,\n",
    "#     )\n",
    "\n",
    "#     return retriever_index_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef3beb6-33ce-4e88-bd7b-f3fb630ac26e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Building and Validating Retriever Index"
    }
   },
   "outputs": [],
   "source": [
    "# retriever_index_result = build_retriever_index(\n",
    "#     # Spark requires `` to escape names with special chars, VS client does not.\n",
    "#     chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE.replace(\"`\", \"\"),\n",
    "#     primary_key=\"chunk_id\",\n",
    "#     embedding_source_column=\"content_chunked\",\n",
    "#     vector_search_endpoint=VECTOR_SEARCH_ENDPOINT,\n",
    "#     vector_search_index_name=VECTOR_INDEX_NAME,\n",
    "#     # Must match the embedding endpoint you used to chunk your documents\n",
    "#     embedding_endpoint_name=EMBEDDING_MODEL_ENDPOINT,\n",
    "#     # Set to true to re-create the vector search endpoint when re-running.\n",
    "#     force_delete_vector_search_endpoint=False,\n",
    "# )\n",
    "\n",
    "# print(retriever_index_result)\n",
    "\n",
    "# print()\n",
    "# print(\"Vector search index created! This will be used in the next notebook.\")\n",
    "# print(f\"Vector search endpoint: {retriever_index_result.vector_search_endpoint}\")\n",
    "# print(f\"Vector search index: {retriever_index_result.vector_search_index_name}\")\n",
    "# print(f\"Embedding used: {retriever_index_result.embedding_endpoint_name}\")\n",
    "# print(f\"Chunked docs table: {retriever_index_result.chunked_docs_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2674917388789749,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Audio_Data_Pipeline_Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
